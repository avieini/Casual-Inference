{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4115, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noamarbe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "PATH = 'movie_metadata.csv'\n",
    "BIN_DURATION = 20\n",
    "THRESHOLD_S_IPW = 15\n",
    "THRESHOLD_S_DR = 30\n",
    "YEAR_DUMP = 1980\n",
    "DURATION_LOW = 80\n",
    "DURATION_HIGH = 190\n",
    "STRATIFICATION_BINS = 5\n",
    "\n",
    "def get_genres(data):\n",
    "    genre_set = set()\n",
    "    for index,row in data.iterrows():\n",
    "        genre_set.update(row['genres'].split('|'))\n",
    "    return genre_set\n",
    "\n",
    "def adjust_budget_gross(data):\n",
    "  inflation = pd.read_csv('inflation_data.csv')\n",
    "  inflation['CPI_Multiplier'] = inflation['CPIAUCNS'].iloc[-1] / inflation['CPIAUCNS']\n",
    "\n",
    "  inflation['YEAR'] = inflation['DATE'].apply(lambda x: int(x[:4]))\n",
    "  data['YEAR'] = data['title_year'].apply(lambda x: int(x))\n",
    "\n",
    "  data = data.join(inflation.set_index('YEAR'), how='left', on='YEAR',lsuffix='_caller', rsuffix='_other')\n",
    "  data = data.drop(columns=['YEAR','DATE','CPIAUCNS'])\n",
    "\n",
    "  data['CPIAdjBudget'] = data['budget']*data['CPI_Multiplier']\n",
    "  #data['CPIAdjGross'] = data['gross']*data['CPI_Multiplier']\n",
    "  data['log_CPIAdjBudget'] = np.log(data['CPIAdjBudget'])\n",
    "  #data['log_CPIAdjGross'] = np.log(data['CPIAdjGross'])\n",
    "\n",
    "  #data['log_CPIAdjProfit'] = data['log_CPIAdjGross']-data['log_CPIAdjBudget']\n",
    "  #data['log_CPIAdjROI'] = (data['log_CPIAdjGross']-data['log_CPIAdjBudget'])/data['log_CPIAdjBudget']\n",
    "  \n",
    "  data = data.drop(columns=['budget','CPIAdjBudget','CPI_Multiplier'],axis=1)\n",
    "  return data\n",
    "\n",
    "\n",
    "def categorize_content_rating(data):\n",
    "  data['content_rating_R'] = np.where(data['content_rating'] == 'R',1,0)\n",
    "  data['content_rating_PG_13'] = np.where(data['content_rating'] == 'PG-13',1,0)\n",
    "  data['content_rating_PG'] = np.where(data['content_rating'] == 'PG',1,0)\n",
    "  data['content_rating_other'] = np.where((data['content_rating'] != 'PG')\n",
    "                                     & (data['content_rating'] != 'PG-13')\n",
    "                                     & (data['content_rating'] != 'R'),1,0)\n",
    "  \n",
    "  data = data.drop(columns=['content_rating'],axis=1)\n",
    "  return data\n",
    " \n",
    "def filter_and_categorize_duration(data,bin_duration):\n",
    "  data = data[(data['duration']>=DURATION_LOW) & (data['duration']<=DURATION_HIGH)]\n",
    "  bins = [i for i in range(DURATION_LOW-1,DURATION_HIGH+1,bin_duration)]\n",
    "  data['bin'],bins = pd.cut(data['duration'],bins,labels=[i for i in range(1,len(bins))],retbins=True)  \n",
    "  return data, bins\n",
    "  \n",
    "  \n",
    "def filter_year(data):\n",
    "  data = data[data['title_year']>=YEAR_DUMP]\n",
    "  return data\n",
    "  \n",
    "def data_preperation(path,bin_duration=10): \n",
    "  data = pd.read_csv(path)\n",
    "  \n",
    "  data.drop_duplicates(inplace=True)\n",
    "  data[\"movie_title\"] = data[\"movie_title\"].str.strip()\n",
    "  data = data.drop(columns=['color','director_name','director_facebook_likes','actor_3_facebook_likes',\n",
    "                       'actor_2_name','actor_1_facebook_likes','actor_1_name',\n",
    "                       'cast_total_facebook_likes','actor_3_name','facenumber_in_poster',\n",
    "                       'plot_keywords','movie_imdb_link','aspect_ratio','movie_facebook_likes',\n",
    "                       'language', 'country','actor_2_facebook_likes','gross'],axis=1)  \n",
    "  data.dropna(subset=['budget','duration','title_year','num_critic_for_reviews'],inplace=True)\n",
    "  \n",
    "  genre_set = get_genres(data)\n",
    "  for genre in genre_set:\n",
    "    data[genre] = np.where(data['genres'].apply(lambda x: x.find(genre) > -1),1,0)\n",
    "  \n",
    "  data = adjust_budget_gross(data)\n",
    "  data = categorize_content_rating(data)\n",
    "  data = filter_year(data)  \n",
    "  data, bins = filter_and_categorize_duration(data,bin_duration)\n",
    "  return data, bins\n",
    "\n",
    "\n",
    "data, bins = data_preperation(PATH,BIN_DURATION)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def propensity_score_lr_predictor(X,T):\n",
    "    clf = LogisticRegression(random_state=0,max_iter=10000, solver='lbfgs',multi_class='multinomial').fit(X, T)    \n",
    "    ps_score = np.asarray(clf.predict_proba(X))\n",
    "    \n",
    "    T_pred = clf.predict(X)\n",
    "    #print('Accuracy LR: ' + str(accuracy_score(T,T_pred)))\n",
    "    \n",
    "    return ps_score\n",
    "\n",
    "def propensity_score_rf_predictor(X,T):\n",
    "    clf = RandomForestClassifier(n_estimators=400, max_depth=5, random_state=0).fit(X,T) \n",
    "    ps_score = np.asarray(clf.predict_proba(X))\n",
    "\n",
    "    T_pred = clf.predict(X)\n",
    "    #print('Accuracy RF: ' + str(accuracy_score(T,T_pred)))\n",
    "    \n",
    "    return np.asarray(clf.predict_proba(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_effect(bin_data,is_treatment=1,is_rct=1):\n",
    "    T = bin_data['T']\n",
    "    y = bin_data['imdb_score']\n",
    "    ps = bin_data['ps_1']\n",
    "    group_record_size = bin_data[bin_data['T']==is_treatment].shape[0]\n",
    "    n = bin_data.shape[0]\n",
    "    group_effect = -1\n",
    "    \n",
    "    if (group_record_size >= THRESHOLD_S_IPW):\n",
    "        if is_rct:\n",
    "            if is_treatment:\n",
    "                group_effect = ((T*y/0.5).sum())/(group_record_size)\n",
    "            else:\n",
    "                group_effect = ((((1-T)*y)/0.5).sum())/(group_record_size)\n",
    "        else:\n",
    "            if is_treatment:\n",
    "                group_effect = (((T*y)/ps).sum())/n\n",
    "            else:\n",
    "                group_effect = ((((1-T)*y)/(1-ps)).sum())/n   \n",
    "    return group_effect\n",
    "    \n",
    "\n",
    "def inverse_propensity_score(bin_data,is_rct=1):\n",
    "    treatment=1\n",
    "    control=0\n",
    "    \n",
    "    treatment_effect = get_group_effect(bin_data,treatment,is_rct) \n",
    "    control_effect = get_group_effect(bin_data,control,is_rct)\n",
    "    \n",
    "    if (treatment_effect != -1 and control_effect != -1):\n",
    "        ate = treatment_effect - control_effect\n",
    "    else:\n",
    "        ate = np.nan\n",
    "    return ate\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "def stratification(bin_data):\n",
    "    ate_scores = []\n",
    "    bin_data = bin_data.sort_values(by=['ps_1'])\n",
    "    bin_data['stratification_bin'] = pd.qcut(bin_data['ps_1'],\n",
    "                                             q=STRATIFICATION_BINS,\n",
    "                                             labels=[i for i in range(1,STRATIFICATION_BINS+1)],\n",
    "                                             retbins=False)\n",
    "    is_rct=1\n",
    "    \n",
    "    for bin in range(1,STRATIFICATION_BINS+1):\n",
    "        inner_bin_data = copy.deepcopy(bin_data[bin_data['stratification_bin']==bin])\n",
    "        ate_scores.append(inverse_propensity_score(inner_bin_data,is_rct))\n",
    "    ate = np.nanmean(ate_scores)\n",
    "    return ate    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy import stats\n",
    "\n",
    "def ridge_linear_regression(adj_bin_data,with_ps=True):\n",
    "    if with_ps:\n",
    "        X = adj_bin_data.drop(columns=['imdb_score','ps_0','ps_1'])\n",
    "    else:\n",
    "        X = X = adj_bin_data.drop(columns=['imdb_score'])\n",
    "        \n",
    "    y = adj_bin_data['imdb_score']\n",
    "    \n",
    "    ridge = Ridge(random_state=0)\n",
    "    \n",
    "    clf = ridge.fit(X, y) \n",
    "    T_coef = clf.coef_[-1]\n",
    "    \n",
    "    y_pred = clf.predict(X)\n",
    "    #y_pred_T = clf.predict(X[X[\"T\"]==1])\n",
    "    #y_pred_C = clf.predict(X[X[\"T\"]==0])\n",
    "    \n",
    "    #ate = (y_pred_T-y_pred_C).mean()\n",
    "    \n",
    "    #print('RMSE: ' + str(math.sqrt(mean_squared_error(y,y_pred))))\n",
    "    \n",
    "    #t_ind,p_value = stats.ttest_ind(y,y_pred,equal_var=False)\n",
    "    #print ('linear_t: ' + str (t_ind) + ' linear_p: '+ str(p_value))\n",
    "\n",
    "    return T_coef, y_pred \n",
    "  \n",
    "def ridge_non_linear_regression(adj_bin_data):\n",
    "    X = adj_bin_data.drop(columns=['imdb_score','ps_0','ps_1'])\n",
    "    y = adj_bin_data['imdb_score']\n",
    "    \n",
    "    columns = X.columns\n",
    "    \n",
    "    ridge = Ridge(random_state=0)\n",
    "    poly = PolynomialFeatures(2)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    clf = ridge.fit(X_poly, y) \n",
    "    features_coef = dict(zip(poly.get_feature_names(columns),clf.coef_))\n",
    "    T_coef_non_linear = features_coef['T']\n",
    "\n",
    "    y_pred = clf.predict(X_poly)\n",
    "    #t_ind,p_value = stats.ttest_ind(y,y_pred,equal_var=False)\n",
    "    #print ('non_linear_t: ' + str (t_ind) + ' non_linear_p: '+ str(p_value))\n",
    "    \n",
    "    #print('RMSE_non_linear: ' + str(math.sqrt(mean_squared_error(y,y_pred))))\n",
    "    return T_coef_non_linear, y_pred \n",
    "    \n",
    "def ridge_non_linear_regression_robust(adj_bin_data):\n",
    "    X = adj_bin_data.drop(columns=['imdb_score','ps_0','ps_1'])\n",
    "    columns = X.columns\n",
    "    \n",
    "    poly = PolynomialFeatures(2)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    X_poly = pd.DataFrame(data=X_poly,columns=poly.get_feature_names(columns))\n",
    "    X_poly_treated = X_poly[X_poly['T']==1]\n",
    "    X_poly_control = X_poly[X_poly['T']==0]\n",
    "    \n",
    "    y_treated = adj_bin_data[adj_bin_data['T']==1]['imdb_score']\n",
    "    y_control = adj_bin_data[adj_bin_data['T']==0]['imdb_score']\n",
    "    \n",
    "    ridge_treated = Ridge(random_state=0)\n",
    "    ridge_control = Ridge(random_state=1)\n",
    "    \n",
    "    clf_treated = ridge_treated.fit(X_poly_treated, y_treated) \n",
    "    clf_control = ridge_control.fit(X_poly_control, y_control) \n",
    "    \n",
    "    y_pred_treated = clf_treated.predict(X_poly)\n",
    "    y_pred_control = clf_control.predict(X_poly)\n",
    "    \n",
    "    #t_ind_treatment,p_value_treatment = stats.ttest_ind(y_treated,y_pred_treated,equal_var=False)\n",
    "    #t_ind_control,p_value_control = stats.ttest_ind(y_control,y_pred_control,equal_var=False)\n",
    "    \n",
    "    #print ('non_linear_robust_treatment_t: ' + str (t_ind_treatment) \n",
    "    #       + ' non_linear_robust_treatment_p: '+ str(p_value_treatment))\n",
    "    #print ('non_linear_robust_control_t: ' + str (t_ind_control) \n",
    "    #       + ' non_linear_robust_control_p: '+ str(p_value_control))\n",
    "    #print('\\n')\n",
    "    return y_pred_treated,y_pred_control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly Robust Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doubly_robust_sum(bin_data):\n",
    "    n = bin_data.shape[0]\n",
    "    T = bin_data['T']\n",
    "    ps = bin_data['ps_1']\n",
    "    treatment_prediction = bin_data['treated_prediction']\n",
    "    control_prediction = bin_data['control_prediction']\n",
    "    y = bin_data['imdb_score']\n",
    "    \n",
    "    left_hand_sum = ((T*y/ps)-((T-ps)/ps)*treatment_prediction).sum()\n",
    "    right_hand_sum = ((((1-T)*y)/(1-ps))+((T-ps)/(1-ps))*control_prediction).sum()\n",
    "    sum = (left_hand_sum - right_hand_sum)/n\n",
    "    return sum\n",
    "\n",
    "def doubly_robust_estimator(bin_data):\n",
    "    n = bin_data.shape[0]\n",
    "    \n",
    "    bin_data_treated = bin_data[bin_data['T']==1]\n",
    "    bin_data_control = bin_data[bin_data['T']==0]\n",
    "    \n",
    "    if (bin_data_treated.shape[0] >= THRESHOLD_S_DR and bin_data_control.shape[0] >= THRESHOLD_S_DR):  \n",
    "        treated_predictions,control_predictions = ridge_non_linear_regression_robust(bin_data)                  \n",
    "                          \n",
    "        bin_data['treated_prediction'] = treated_predictions\n",
    "        bin_data['control_prediction'] = control_predictions\n",
    "\n",
    "        ate = calc_doubly_robust_sum(bin_data)\n",
    "    else:\n",
    "        ate = np.nan\n",
    "    return ate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def sensitivity_analysis(adj_bin_data,T_coef_linear):\n",
    "    adj_bin_data['error'] = adj_bin_data['imdb_score'] - adj_bin_data['imdb_score_predicted']\n",
    "    var_error_treated = adj_bin_data[adj_bin_data['T']==1]['error'].var()\n",
    "    var_error_control = adj_bin_data[adj_bin_data['T']==0]['error'].var()\n",
    "    \n",
    "    a=2\n",
    "    step=0.05\n",
    "    \n",
    "    lamda_values = np.arange(start=-a,stop=a+step,step=step)\n",
    "    ate_values = []\n",
    "    \n",
    "    for lamda in lamda_values:\n",
    "        rho_1 = lamda*var_error_treated\n",
    "        bias_1_col = adj_bin_data['ps_0']*rho_1\n",
    "        bias_1 = bias_1_col.mean()\n",
    "        \n",
    "        rho_0 = lamda*var_error_control\n",
    "        bias_0_col = adj_bin_data['ps_1']*rho_0.mean()\n",
    "        bias_0 = bias_0_col.mean()\n",
    "        \n",
    "        res = T_coef_linear - bias_1 - bias_0\n",
    "        ate_values.append(res)\n",
    "    \n",
    "    index = -1\n",
    "    min_value = min(np.absolute(ate_values))\n",
    "    if min_value in ate_values:\n",
    "        index = ate_values.index(min_value)\n",
    "    else:\n",
    "        index = ate_values.index(min_value*(-1))\n",
    "                    \n",
    "    tipping_point = lamda_values[index]\n",
    "    \n",
    "    \n",
    "    \"\"\"plt.plot(lamda_values,ate_values)\n",
    "    blue_patch = mpatches.Patch(color='blue', label='ATE')\n",
    "    plt.legend(handles=[blue_patch])\n",
    "    plt.ylim(-2,2)\n",
    "    plt.xlim(-(a+step),a+step)\n",
    "    plt.axhline(y=0,linestyle=':')\n",
    "    plt.show()\n",
    "    plt.close()\"\"\"\n",
    "    \n",
    "    rho_1_f = tipping_point*var_error_treated\n",
    "    rho_0_f = tipping_point*var_error_control\n",
    "    #print(rho_1_f)\n",
    "    #print(rho_0_f)\n",
    "    return rho_0_f,rho_1_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3037976860614079\n",
      "0.11805530917996435\n",
      "0.0636306929995773\n",
      "0.5720116373899895\n",
      "-0.0126110863887734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "results_dict = dict()\n",
    "stratification_ate_list = list()\n",
    "linear_regression_estimator_list = list()\n",
    "non_linear_regression_estimator_list = list()\n",
    "inverse_ps_list = list()\n",
    "doubly_robust_list = list()\n",
    "\n",
    "for bin in range(1,len(bins)-1): \n",
    "    bin_data = copy.deepcopy(data[(data['bin']==bin) | (data['bin']==bin+1)])\n",
    "    \n",
    "    X = bin_data.drop(columns=['imdb_score','bin','duration','movie_title','genres'],axis=1) \n",
    "    columns = X.columns\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(data=X,columns=columns)\n",
    "\n",
    "    bin_data['T'] = np.where(bin_data['bin']==bin,0,1)\n",
    "    T = bin_data['T']\n",
    "    \n",
    "    y = bin_data['imdb_score']\n",
    "    \n",
    "    propensity_score_lr = propensity_score_lr_predictor(X,T)\n",
    "    propensity_score = pd.DataFrame(data={'ps_0':propensity_score_lr[:,0],\n",
    "                                          'ps_1':propensity_score_lr[:,1]},\n",
    "                                   columns=['ps_0','ps_1'])   \n",
    "    \n",
    "    T = T.reset_index()\n",
    "    propensity_score = propensity_score.reset_index()\n",
    "    y = y.reset_index()\n",
    "    \n",
    "    adj_bin_data = pd.concat([X,propensity_score,T,y],axis=1)\n",
    "    adj_bin_data = adj_bin_data.drop(columns=['index'],axis=1)\n",
    "\n",
    "    stratification_ate_list.append(stratification(adj_bin_data))\n",
    "                                                                        \n",
    "    T_coef_non_linear, _ = ridge_non_linear_regression(adj_bin_data)\n",
    "    T_coef_linear, y_pred = ridge_linear_regression(adj_bin_data)\n",
    "    linear_regression_estimator_list.append(T_coef_linear)\n",
    "    non_linear_regression_estimator_list.append(T_coef_non_linear)\n",
    "    \n",
    "    adj_bin_data['imdb_score_predicted'] = y_pred\n",
    "    tipping_point = sensitivity_analysis(adj_bin_data,T_coef_linear)\n",
    "    \n",
    "    is_rct = 0\n",
    "    inverse_ps_list.append(inverse_propensity_score(adj_bin_data,is_rct))\n",
    "    \n",
    "    doubly_robust_list.append(doubly_robust_estimator(adj_bin_data))\n",
    "    \n",
    "stratification_ate = np.nanmean(stratification_ate_list)\n",
    "stratification_ate_list.append(stratification_ate)\n",
    "results_dict['stratification'] = stratification_ate_list\n",
    "\n",
    "linear_regression_ate = np.mean(linear_regression_estimator_list)\n",
    "linear_regression_estimator_list.append(linear_regression_ate)\n",
    "non_linear_regression_ate = np.mean(non_linear_regression_estimator_list)\n",
    "non_linear_regression_estimator_list.append(non_linear_regression_ate)\n",
    "results_dict['linear_regression'] = linear_regression_estimator_list\n",
    "results_dict['non_linear_regression'] = non_linear_regression_estimator_list\n",
    "\n",
    "inverse_ps_ate = np.nanmean(inverse_ps_list)\n",
    "inverse_ps_list.append(inverse_ps_ate)\n",
    "results_dict['inverse_ps'] = inverse_ps_list\n",
    "\n",
    "doubly_robust_ate = np.nanmean(doubly_robust_list)\n",
    "doubly_robust_list.append(doubly_robust_ate)\n",
    "results_dict['doubly_robust'] = doubly_robust_list\n",
    "\n",
    "print(stratification_ate)\n",
    "print(linear_regression_ate)\n",
    "print(non_linear_regression_ate)\n",
    "print(inverse_ps_ate)\n",
    "print(doubly_robust_ate)\n",
    "\n",
    "#esults = pd.DataFrame(data=results_dict)\n",
    "#results.to_csv('results_%d_min.csv'%(BIN_DURATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
